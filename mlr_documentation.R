library(mlr)
library(mlr3)
library(mlbench)
library(tidyverse)

# standardized interface for machine learning techniques and resampling, 
# parallelization, performance evaluation etc

################# 1. TASKS
# Learning tasks encapsulate the data set and further relevant information about a machine learning problem, 
# for example the name of the target variable for supervised problems.

# call make<TaskType>
# Depending on the nature of the learning problem, additional arguments may be required

# regression
data(BostonHousing, package = "mlbench")

regr.task <- makeRegrTask(id = "bh", data = BostonHousing, target = "medv")
regr.task

########### classification
# the target column has to be a FACTOR

data(BreastCancer, package="mlbench")
df <- BreastCancer
df$Id <- NULL

classif.task <- makeClassifTask(id="BreastCancer", data=df, target="Class")
classif.task

# In binary classification the two classes are usually referred to as positive and negative class
# with the positive class being the category of greater interest. 
# This is relevant for many performance measures like the true positive rate or ROC analysis. 

### IMPORTANT
# Moreover, mlr, where possible, permits to set options (like the setThreshold() or makeWeightedClassesWrapper()) and returns and plots results 
# (like class posterior probabilities) for the positive class only.

# makeClassifTask() by default selects the first factor level of the target variable as the positive class, in the above example benign. 
# Class malignant can be manually selected as follows:
classif.task = makeClassifTask(id = "BreastCancer", data = df, target = "Class", positive = "malignant")

###### Cluster Analysis

data(mtcars, package = "datasets")
cluster.task <- makeClusterTask(data = mtcars)
cluster.task

###### Accessing a learning task
getTaskDesc(cluster.task)

# Get the ID
getTaskId(classif.task)

# Get the type of task
getTaskType(classif.task)

# Get the names of the target columns
getTaskTargetNames(classif.task)

# Get the number of observations
getTaskSize(classif.task)

# Get the number of input variables
getTaskNFeats(classif.task)

# Get the class levels in classif.task
getTaskClassLevels(classif.task)


str(getTaskData(classif.task))
getTaskFeatureNames(cluster.task)

# you can also modify an exisiting task

################# 2. LEARNERS
# here you choose the ML algorithm that you want to use to carry out a task
listLearners()$class
# A learner in mlr is generated by calling makeLearner(). 
# In the constructor you need to specify which learning method you want to use.Moreover, you can:
#  - Set hyperparameters.
#  - Control the output for later prediction, e.g., for classification whether you want a factor of predicted class labels or probabilities.
#  - Set an ID to name the object (some methods will later use this ID to name results or annotate plots).

classif.lrn <- makeLearner("classif.randomForest", predict.type = "prob", fix.factors.prediction = T)

regr.lrn <- makeLearner("regr.gbm", par.vals = list(n.trees = 500, interaction.depth = 3))

cluster.lrn <-  makeLearner("cluster.kmeans", centers = 5)

# Hyperparameter values can be specified as a list via par.vals
# fix.factor.prediction:
# Occasionally, factor features may cause problems when fewer levels are present in the test data set than in the training data. 
# By setting fix.factors.prediction = TRUE these are avoided by adding a factor level for missing data in the test data set.

# We can also use getParamSet() or its alias getLearnerParamSet() to get a quick overview about the available hyperparameters 
# and defaults of a learning method without explicitly constructing it (by calling makeLearner()).
getParamSet("classif.randomForest")

################# 3. TRAINING A LEARNER ON A TASK
# training the learner means to fit a model into a given dataset.
# in mlr you do it by calling the funciton train() on a learner(makeLearner()) and a suitable task
# linear discriminant analysis:
task <- makeClassifTask(data = iris, target = "Species")
lrn <- makeLearner("classif.lda")
# now train the learner. You create a MODEL
mod <- train(lrn, task)

# creating the learner is nit necessary, you specify it in the train() function right away.
mod <- train("classif.lda", task)

# Function train() returns an object of class WrappedModel (makeWrappedModel()), which encapsulates the fitted model,
# i.e., the output of the underlying R learning method. 
# Additionally, it contains some information about the Learner (makeLearner()), the Task(), the features and observations used for training, and the training time. 
# A WrappedModel (makeWrappedModel()) can subsequently be used to make a prediction (predict.WrappedModel()) for new observations.
# The fitted model in slot $learner.model of the WrappedModel (makeWrappedModel()) object can be accessed using function getLearnerModel.
# In the following example we cluster the Ruspini (cluster::ruspini()) data set (which has four groups and two features) by K-means with K=4 and extract the output of the underlying stats::kmeans() function.

data(ruspini, package = "cluster")
qplot(data= ruspini, x = x, y = y, geom = "point")

ruspini.task <- makeClusterTask(data = ruspini)
lnr <- makeLearner("cluster.kmeans", par.vals = list(centers=4))
mod <- train(lnr, ruspini.task)

# with getLearnerModel you get the fitted model (good for interpreting the data)
getLearnerModel(mod)
ruspini$clusters <- mod$learner.model$cluster
ruspini$clusters <- as.factor(ruspini$clusters)

ggplot(data=ruspini, aes(x=x, y=y, colour=clusters)) + 
  geom_point(size=2)

# By default, the whole data set in the Task() is used for training.
# The subset argument of train() takes a logical or integer vector that indicates which observations to use, 
# for example if you want to split your data into a training and a test set... 
# ... or if you want to fit separate models to different subgroups in the data.

# Get the number of observations
n = getTaskSize(bh.task)
n
# Use 1/3 of the observations for training
train.set = sample(n, size = n/3)

# Train the learner
mod = train("regr.lm", bh.task, subset = train.set)
mod

# in any case, you do not have to split the data by yourself, many resample stratgies are supported by mlr











